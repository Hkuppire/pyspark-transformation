# PySpark Data Transformation

This project showcases a simple PySpark workflow for reading raw data, cleaning, transforming, and writing it into Parquet format.

## ğŸš€ What This Project Does

- Reads CSV data using PySpark  
- Removes duplicates  
- Filters invalid values  
- Creates a new computed column  
- Writes clean output to Parquet  

## ğŸ“ File Overview

### `transform_data.py`
Main PySpark script that performs:
- Data cleaning  
- Column transformations  
- Filtering  
- Output writing  

## ğŸ›  Technologies Used
- PySpark  
- Apache Spark  
- Parquet  
- Python  

## ğŸ¯ Purpose
This project demonstrates core data engineering skills:
- Working with distributed data  
- Transformations at scale  
- Efficient file formats (Parquet)  
- Spark pipeline design  

